---
title: "Modeling"
author: "David John Baker"
date: "12/23/2018"
output: html_document
---

## Plots and Modeling

Using our data from before, let's now try to make a plot where we predict X from Y.
First let's make this with ggplot2.

```{r}
library(tidyverse)

txhousing %>%
  filter(city == "Paris")

txhousing %>%
  filter(city == "Paris") %>%
  ggplot(aes(x = year , y = sales)) + geom_point()

txhousing %>%
  filter(city == "Paris") %>%
  ggplot(aes(x = year , y = sales)) + geom_point() + 
  labs(title = "City of Paris Sales by Year", x = "Year", y = "Sales")


txhousing %>%
  filter(city == "Paris") %>%
  ggplot(aes(x = year , y = sales)) + geom_point() + 
  labs(title = "City of Paris Sales by Year", x = "Year", y = "Sales") +
  theme_classic()

txhousing %>%
#  filter(city == "Paris") %>%
  ggplot(aes(x = year , y = sales, color = city)) + geom_point() + 
  labs(title = "City of Paris Sales by Year", x = "Year", y = "Sales") +
  theme_classic()

txhousing %>%
  ggplot(aes(x = year, y = listings)) + geom_point() +
  facet_wrap(~city)

txhousing %>%
  group_by(city) %>%
  mutate(zListings = scale(listings)) %>%
  ungroup(city) %>%
  ggplot(aes(x = year, y = zListings)) + geom_point() +
  facet_wrap(~city)

```


## Basic Linear Models 

Linear models are done with the ```lm()``` function and follow the general pattern of

> model <- lm(DV ~ IV, data = yourData)
> summary(model)

Models are saved as lists meaning you can access any part of the model with the ```$``` command on the object name.
Let's try to run some models on our Texas Housing Data.

First, together let's run a model where we try to predict DV from IV.
Once saving this to an object, we can get the output from ```summary()```.
Let's try to predict 

```{r}
options(scipen = 999) # Remove Scientfic notation from R 
model1 <- lm(sales ~ listings, data = txhousing)
summary(model1)
```


## Model Interpretation

The Call lets us know what we put into R.
Our coeffecients tell us the degree to which one variable effects another.
The estimates are our intercept and beta-coeffecient (non-standardized!).
For everyone one unit increase in our IV (listings) we can expect a .179 rise in our DV (sales) in whatever unit they are!
Your test statistics are to the right of that.
Standard errors are calculated on your estimate (how much error there is on either side)
You t-value is you test statistic.
Your p value is the probability of getting a non-zero value given the null hypothesis that there is no effect.
Degrees of freedom are listd below.
Your multiple-R squared tells you how much variance you explain.

We can then add more variables to try to improve our predition.
For example, what happens when we add in the year as a variable?

```{r}
model2 <- lm(sales ~ listings + year, data = txhousing)
summary(model2)
```

Here we also find a significant effect of year and listing on sales.
Our model fit improves a very small amount, yet the difference is significant.
This *significance* should be interpreted with caution as we have a lot of data and the more data you have, the more likely a result is to come up as p <.01.

We can also test the degree that one model is better than another with the ```anova()``` function.

```{r}
anova(model1, model2)
```

## Own Analyses

For the rest of the class, I want everyone to try their own plotting and regression models.
Use your own data, use data provided by R. 

## Future

In the future, check out Hadely Wickham's [R For Data Science](https://r4ds.had.co.nz/) which will give you a solid introduction to R and all it can do.
Much of your time will also be spent on Stack Overflow.
I also very much suggest setting up a Facebook group or something of people here that are about on the same skill level so that you can help each other out.
